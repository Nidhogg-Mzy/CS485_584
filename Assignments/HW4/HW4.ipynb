{"cells":[{"cell_type":"markdown","id":"520c64d2","metadata":{"id":"520c64d2"},"source":["# Homework 4\n","In this homework, we will cover panorama stitching.\n","\n","*This homework was adapted from Stanford CS131.*"]},{"cell_type":"markdown","id":"GWFsmOnsoA1J","metadata":{"id":"GWFsmOnsoA1J"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"id":"e4a3223b","metadata":{"id":"e4a3223b"},"outputs":[],"source":["import os\n","\n","if not os.path.exists(\"CS485_584\"):\n","    # Clone the repository if it doesn't already exist\n","    !git clone https://github.com/YoungjoongEmory/CS485_584.git\n","\n","%cd CS485_584/Assignments/HW4/"]},{"cell_type":"code","execution_count":null,"id":"f6b81ccb","metadata":{"id":"f6b81ccb"},"outputs":[],"source":["# Install the necessary dependencies\n","# (restart your runtime session if prompted to, and then re-run this cell)\n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"id":"CZeVp0GlYHzF","metadata":{"id":"CZeVp0GlYHzF"},"outputs":[],"source":["from __future__ import print_function\n","\n","import numpy as np\n","from skimage import filters\n","from skimage.feature import corner_peaks\n","from skimage.io import imread\n","from scipy.spatial.distance import cdist\n","from scipy.ndimage import convolve\n","from scipy import ndimage\n","import matplotlib.pyplot as plt\n","import cv2 as cv\n","\n","from utils import pad, unpad, get_output_space, warp_image, plot_matches, describe_keypoints\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (12.0, 9.0)\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'"]},{"cell_type":"markdown","id":"aKO0T9OFKS1I","metadata":{"id":"aKO0T9OFKS1I"},"source":["# Panorama Stitching"]},{"cell_type":"markdown","id":"77e91e1c","metadata":{"id":"77e91e1c"},"source":["Panorama stitching is an early success of computer vision. Matthew Brown and David G. Lowe published a famous [panoramic image stitching paper](http://matthewalunbrown.com/papers/ijcv2007.pdf) in 2007. Since then, automatic panorama stitching technology has been widely adopted in many applications such as Google Street View, panorama photos on smartphones,\n","and stitching software such as Photosynth and AutoStitch.\n","\n","**We will detect and match keypoints from multiple images to build a single panoramic image. This will involve several tasks:**\n","1. Compare two sets of descriptors coming from two different images and find matching keypoints.\n","2. Given matching keypoints, use least-squares to find the affine transformation matrix that maps points in one image to another.\n","3. Use RANSAC to give a more robust estimate of the affine transformation matrix. <br>\n","Given the transformation matrix, use it to transform the second image and overlay it on the first image, forming a panorama.\n","4. Blend panorama images together to remove blurry regions of overlapping images.\n"]},{"cell_type":"markdown","id":"X2MJpP4xfQXw","metadata":{"id":"X2MJpP4xfQXw"},"source":["## 1. Describing and Matching Keypoints (0 points)\n","We will use Harris corner detection to detect keypoints in two images. Given these two sets of keypoints, we will then determine which pairs of keypoints come from the same 3D points projected onto the two images. We do so by first converting the region around each keypoint into a descriptor. Then, we find good matches in the two sets of descriptors based on Euclidean distance.\n","\n","Use the following definition of cornerness response:\n","$$\n","\\theta = \\det(M) - \\alpha (\\operatorname{trace}(M))^2\n","= (w * I_x^2)(w * I_y^2) - (w * (I_x I_y))^2 - \\alpha (w * (I_x^2) + w * (I_y^2))^2\n","$$\n","\n","where \\( * \\) denotes convolution and \\( w \\) is the window function.\n"]},{"cell_type":"code","execution_count":null,"id":"P1qLHP2Tfsf5","metadata":{"id":"P1qLHP2Tfsf5"},"outputs":[],"source":["def harris_corners(img, window_size=3, k=0.04):\n","    \"\"\"\n","    Compute Harris corner response map. Follow the math equation\n","    R=Det(M)-k(Trace(M)^2).\n","    Hint:\n","        You may use the function ndimage.convolve,\n","        which is already imported above. If you use convolve(), remember to\n","        specify zero-padding to match our equations, for example:\n","\n","            out_image = ndimage.convolve(in_image, kernel, mode='constant', cval=0)\n","\n","    Args:\n","        img: Grayscale image of shape (H, W)\n","        window_size: size of the window function\n","        k: sensitivity parameter\n","\n","    Returns:\n","        response: Harris response image of shape (H, W)\n","    \"\"\"\n","\n","    H, W = img.shape\n","    window = np.ones((window_size, window_size)) # window function\n","\n","    response = np.zeros((H, W))\n","\n","    # 1. Compute x and y derivatives (I_x, I_y) of an image\n","    dx = filters.sobel_v(img)\n","    dy = filters.sobel_h(img)\n","\n","    ### YOUR CODE HERE\n","\n","    # Compute products of derivatives using dx and dy.\n","    Ixx = dx * dx\n","    Iyy = ...\n","    Ixy = ...\n","\n","    # Sum of products within the window (use zero padding)\n","    Sxx = ndimage.convolve(Ixx, window, mode='constant', cval=0)\n","    Syy = ...\n","    Sxy = ...\n","\n","    # Harris response: R = det(M) - k * (trace(M))^2\n","    detM = ...\n","    traceM = ...\n","    response = detM - k * (traceM ** 2)\n","\n","    ### END YOUR CODE\n","\n","    return response\n","\n","def simple_descriptor(patch):\n","    \"\"\"\n","    Describe the patch by normalizing the image values into a standard\n","    normal distribution (having mean of 0 and standard deviation of 1)\n","    and then flattening into a 1D array.\n","\n","    The normalization will make the descriptor more robust to change\n","    in lighting condition.\n","\n","    Hint:\n","        In this case of normalization, if a denominator is zero, divide by 1 instead.\n","\n","    Args:\n","        patch: grayscale image patch of shape (H, W)\n","\n","    Returns:\n","        feature: 1D array of shape (H * W)\n","    \"\"\"\n","\n","    feature = []\n","    patch = patch.astype(np.float32)\n","    mu = np.mean(patch)\n","    sigma = np.std(patch)\n","\n","    ### YOUR CODE HERE\n","\n","\n","\n","    ### END YOUR CODE\n","\n","    feature = norm.flatten()\n","\n","    return feature\n","\n","def match_descriptors(desc1, desc2, threshold=0.5):\n","    \"\"\"\n","    Match the feature descriptors by finding distances between them. A match is formed\n","    when the distance to the closest vector is much smaller than the distance to the\n","    second-closest, that is, the ratio of the distances should be STRICTLY SMALLER\n","    than the threshold (NOT equal to). Return the matches as pairs of vector indices.\n","\n","    Hint:\n","        The Numpy functions np.sort, np.argmin, np.asarray might be useful\n","\n","        The Scipy function cdist calculates Euclidean distance between all\n","        pairs of inputs\n","    Args:\n","        desc1: an array of shape (M, P) holding descriptors of size P about M keypoints\n","        desc2: an array of shape (N, P) holding descriptors of size P about N keypoints\n","\n","    Returns:\n","        matches: an array of shape (Q, 2) where each row holds the indices of one pair\n","        of matching descriptors\n","    \"\"\"\n","\n","    matches = []\n","\n","    M = desc1.shape[0]\n","\n","    # dists is (M, N); each row corresponds to one descriptor from image 1\n","    # and distances to all descriptors in image 2\n","    dists = cdist(desc1, desc2)\n","\n","    # For each descriptor in desc1, apply ratio test against desc2\n","\n","    # dists is (M, N); for each row, find indices of two smallest distances\n","    idx_sorted = np.argsort(dists, axis=1)\n","    nearest = idx_sorted[:, 0]\n","    second_nearest = idx_sorted[:, 1]\n","\n","    # Gather corresponding smallest and second smallest distances\n","    d1 = dists[np.arange(M), nearest]\n","    d2 = dists[np.arange(M), second_nearest]\n","\n","    # Ratio test\n","    valid = d1 / (d2 + 1e-12) < threshold\n","    for i in np.where(valid)[0]:\n","        matches.append([i, nearest[i]])\n","\n","    matches = np.asarray(matches, dtype=int) if len(matches) > 0 else np.zeros((0, 2), dtype=int)\n","\n","    return matches"]},{"cell_type":"markdown","id":"4d6c2899","metadata":{"id":"4d6c2899"},"source":["Run the following cell to detect and match the keypoints in two images."]},{"cell_type":"code","execution_count":null,"id":"8aea9b37","metadata":{"id":"8aea9b37"},"outputs":[],"source":["img1 = imread('uttower1.jpg', as_gray=True)\n","img2 = imread('uttower2.jpg', as_gray=True)\n","\n","np.random.seed(131)\n","\n","# Detect keypoints in two images\n","keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n","                          threshold_rel=0.05,\n","                          exclude_border=8)\n","keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n","                          threshold_rel=0.05,\n","                          exclude_border=8)\n","\n","# Extract features from the corners\n","desc1 = describe_keypoints(img1, keypoints1,\n","                           desc_func=simple_descriptor,\n","                           patch_size=5)\n","desc2 = describe_keypoints(img2, keypoints2,\n","                           desc_func=simple_descriptor,\n","                           patch_size=5)\n","\n","# Match descriptors in image1 to those in image2\n","matches = match_descriptors(desc1, desc2, 0.7)"]},{"cell_type":"markdown","id":"3deb1bea","metadata":{"id":"3deb1bea"},"source":["## 2. Transformation Estimation (20 points)\n","\n","Now, we will use these matched keypoints to find a **transformation matrix** that maps points in the second image to the corresponding coordinates in the first image. In other words, if the point $p_1 = [y_1,x_1]$ in image 1 matches with $p_2=[y_2, x_2]$ in image 2, we need to find an affine transformation matrix $H$ such that\n","\n","$$\n","\\tilde{p_2}H = \\tilde{p_1},\n","$$\n","\n","where $\\tilde{p_1}$ and $\\tilde{p_2}$ are homogenous coordinates of $p_1$ and $p_2$.\n","\n","Note that it may be impossible to find the transformation $H$ that maps every point in image 2 exactly to the corresponding point in image 1. However, **we can estimate the transformation matrix with the least squares method.** Given $N$ matched keypoint pairs, let $X_1$ and $X_2$ be $N \\times 3$ matrices whose rows are homogenous coordinates of corresponding keypoints in image 1 and image 2 respectively. Then, we can estimate $H$ by solving the least squares problem,\n","\n","$$\n","X_2 H = X_1\n","$$\n","\n","Implement **`fit_affine_matrix`** below.\n","\n","*Hint: read the [documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html) for `np.linalg.lstsq`*"]},{"cell_type":"code","execution_count":null,"id":"3CjJ9hsGY8nY","metadata":{"id":"3CjJ9hsGY8nY"},"outputs":[],"source":["def fit_affine_matrix(p1, p2, to_pad=True):\n","    \"\"\"\n","    Fit affine matrix such that p2 * H = p1. First, pad the descriptor vectors\n","    with a 1 using pad() to convert to homogeneous coordinates, then return\n","    the least squares fit affine matrix in homogeneous coordinates.\n","\n","    Hint:\n","        You can use np.linalg.lstsq function to solve the problem.\n","\n","        Explicitly specify np.linalg.lstsq's new default parameter rcond=None\n","        to suppress deprecation warnings.\n","\n","    Args:\n","        p1: an array of shape (M, P) holding descriptors of size P about M keypoints\n","        p2: an array of shape (M, P) holding descriptors of size P about M keypoints\n","\n","    Return:\n","        H: a matrix of shape (P+1, P+1) that transforms p2 to p1 in homogeneous\n","        coordinates\n","    \"\"\"\n","\n","    assert (p1.shape[0] == p2.shape[0]),\\\n","        'Different number of points in p1 and p2'\n","\n","    if to_pad:\n","        p1 = pad(p1)\n","        p2 = pad(p2)\n","\n","    ### YOUR CODE HERE\n","\n","\n","    ### END YOUR CODE\n","\n","    # Sometimes numerical issues cause least-squares to produce the last\n","    # column which is not exactly [0, 0, 1]\n","    H[:,2] = np.array([0, 0, 1])\n","    return H"]},{"cell_type":"code","execution_count":null,"id":"6e5df1aa","metadata":{"id":"6e5df1aa"},"outputs":[],"source":["# Sanity check for fit_affine_matrix\n","\n","# Test inputs\n","a = np.array([[0.5, 0.1], [0.4, 0.2], [0.8, 0.2]])\n","b = np.array([[0.3, -0.2], [-0.4, -0.9], [0.1, 0.1]])\n","\n","H = fit_affine_matrix(b, a)\n","\n","# Target output\n","sol = np.array(\n","    [[1.25, 2.5, 0.0],\n","     [-5.75, -4.5, 0.0],\n","     [0.25, -1.0, 1.0]]\n",")\n","\n","error = np.sum((H - sol) ** 2)\n","\n","if error < 1e-20:\n","    print('Implementation correct!')\n","else:\n","    print('There is something wrong.')"]},{"cell_type":"markdown","id":"00474282","metadata":{"id":"00474282"},"source":["After checking that your **`fit_affine_matrix`** function is running correctly, run the following code to apply it to images.\n","\n","Images will be warped and image 2 will be mapped to image 1."]},{"cell_type":"code","execution_count":null,"id":"f32c7873","metadata":{"id":"f32c7873"},"outputs":[],"source":["p1 = keypoints1[matches[:,0]]\n","p2 = keypoints2[matches[:,1]]\n","\n","# Find affine transformation matrix H that maps p2 to p1\n","H = fit_affine_matrix(p1, p2)\n","\n","output_shape, offset = get_output_space(img1, [img2], [H])\n","print(\"Output shape:\", output_shape)\n","print(\"Offset:\", offset)\n","\n","# Warp images into output space\n","img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n","img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n","img1_warped[~img1_mask] = 0     # Return background values to 0\n","\n","img2_warped = warp_image(img2, H, output_shape, offset)\n","img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n","img2_warped[~img2_mask] = 0     # Return background values to 0\n","\n","# Plot warped images\n","plt.subplot(1,2,1)\n","plt.imshow(img1_warped)\n","plt.title('Image 1 Warped')\n","plt.axis('off')\n","\n","plt.subplot(1,2,2)\n","plt.imshow(img2_warped)\n","plt.title('Image 2 Warped')\n","plt.axis('off')\n","\n","plt.show()"]},{"cell_type":"markdown","id":"c88b60e5","metadata":{"id":"c88b60e5"},"source":["Next, the two warped images are merged to get a panorama.\n","\n","Your panorama may not look good at this point, but we will later use other techniques to get a better result!"]},{"cell_type":"code","execution_count":null,"id":"93c9763f","metadata":{"id":"93c9763f"},"outputs":[],"source":["# Merge the two images\n","merged = img1_warped + img2_warped\n","\n","# Track the overlap by adding the masks together\n","overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n","           img2_mask)\n","\n","# Normalize through division by `overlap` - but ensure the minimum is 1\n","normalized = merged / np.maximum(overlap, 1)\n","\n","plt.imshow(normalized)\n","plt.axis('off')\n","plt.title('Fit-Affine Panorama')\n","plt.show()\n","\n","plt.imshow(imread('solution_fit_affine_panorama.png'))\n","plt.axis('off')\n","plt.title('Fit-Affine Panorama Solution')\n","plt.show()"]},{"cell_type":"markdown","id":"c1294483","metadata":{"id":"c1294483"},"source":["## 3. RANSAC (20 points)\n","Rather than directly feeding all our keypoint matches into **``fit_affine_matrix``**, we can use **RANSAC** (\"RANdom SAmple Consensus\") to select only \"inliers\" to use for computing the transformation matrix.\n","\n","Use Euclidean distance as a measure of inliers vs. outliers.\n","\n","\n","The steps of RANSAC are:\n","1. Select random set of matches\n","2. Compute affine transformation matrix\n","      - You can call your **`fit_affine_matrix`** function for this. **Make sure to explicitly pass in `to_pad=False`.**\n","3. Find inliers using the given threshold\n","4. Repeat and keep the largest set of inliers (use >, i.e. break ties by whichever set is seen first)\n","5. Re-compute least-squares estimate on all of the inliers\n","\n","Implement **`ransac`** below."]},{"cell_type":"code","execution_count":null,"id":"Uuz16CSCjdL7","metadata":{"id":"Uuz16CSCjdL7"},"outputs":[],"source":["def ransac(keypoints1, keypoints2, matches, n_iters=200, threshold=20):\n","    \"\"\"\n","    Use RANSAC to find a robust affine transformation:\n","\n","        1. Select random set of matches\n","        2. Compute affine transformation matrix\n","        3. Compute inliers via Euclidean distance\n","        4. Keep the largest set of inliers (use >, i.e. break ties by whichever set is seen first)\n","        5. Re-compute least-squares estimate on all of the inliers\n","\n","    Update max_inliers as a boolean array where True represents the keypoint\n","    at this index is an inlier, while False represents that it is not an inlier.\n","\n","    Hint:\n","        You can use fit_affine_matrix to compute the affine transformation matrix.\n","        Make sure to pass in to_pad=False, since we pad the matches for you here.\n","\n","        You can compute elementwise boolean operations between two numpy arrays,\n","        and use boolean arrays to select array elements by index:\n","        https://numpy.org/doc/stable/reference/arrays.indexing.html#boolean-array-indexing\n","\n","    Args:\n","        keypoints1: M1 x 2 matrix, each row is a point\n","        keypoints2: M2 x 2 matrix, each row is a point\n","        matches: N x 2 matrix, each row represents a match\n","            [index of keypoint1, index of keypoint 2]\n","        n_iters: the number of iterations RANSAC will run\n","        threshold: the threshold to find inliers\n","\n","    Returns:\n","        H: a robust estimation of affine transformation from keypoints2 to\n","        keypoints 1\n","    \"\"\"\n","\n","    # Copy matches array, to avoid overwriting it\n","    orig_matches = matches.copy()\n","    matches = matches.copy()\n","\n","    N = matches.shape[0]\n","    n_samples = int(N * 0.2)\n","\n","    matched1 = pad(keypoints1[matches[:,0]])\n","    matched2 = pad(keypoints2[matches[:,1]])\n","\n","    max_inliers = np.zeros(N, dtype=bool)\n","    n_inliers = 0\n","\n","    # RANSAC iteration start\n","\n","    # Note: while there're many ways to do random sampling, we use\n","    # `np.random.shuffle()` followed by slicing out the first `n_samples`\n","    # matches here in order to align with the autograder.\n","    # Sample with this code:\n","    for i in range(n_iters):\n","        # 1. Select random set of matches\n","        np.random.shuffle(matches)\n","        samples = matches[:n_samples]\n","        sample1 = pad(keypoints1[samples[:,0]])\n","        sample2 = pad(keypoints2[samples[:,1]])\n","\n","        ### YOUR CODE HERE\n","\n","        # 2. Compute affine transformation matrix, map sample2 to sample1\n","\n","        # 3. Compute inliers via Euclidean distance\n","\n","        # 4. Keep the largest set of inliers\n","\n","        ### END YOUR CODE\n","\n","    # 5. Re-compute least-squares estimate on all of the inliers\n","    if max_inliers.any():\n","        H, _ = fit_affine_matrix(matched1[max_inliers], matched2[max_inliers], to_pad=False), None\n","    else:\n","        # Fallback: use identity if no inliers found\n","        H = np.eye(3)\n","\n","\n","    return H, orig_matches[max_inliers]"]},{"cell_type":"markdown","id":"_sjjRshYjdYx","metadata":{"id":"_sjjRshYjdYx"},"source":["Now, run through the following cells to get a panorama. You'll be able to see the difference from the result we got before without RANSAC."]},{"cell_type":"code","execution_count":null,"id":"525334d1","metadata":{"id":"525334d1"},"outputs":[],"source":["# Set seed to compare output against solution image\n","np.random.seed(131)\n","\n","H, robust_matches = ransac(keypoints1, keypoints2, matches, threshold=1)\n","print(\"Robust matches shape = \", robust_matches.shape)\n","print(\"H = \\n\", H)\n","\n","# Visualize robust matches\n","fig, ax = plt.subplots(1, 1, figsize=(12, 9))\n","plot_matches(ax, img1, img2, keypoints1, keypoints2, robust_matches)\n","plt.axis('off')\n","plt.title('RANSAC Robust Matches')\n","plt.show()\n","\n","plt.imshow(imread('solution_ransac.png'))\n","plt.axis('off')\n","plt.title('RANSAC Robust Matches Solution')\n","plt.show()"]},{"cell_type":"markdown","id":"048db598","metadata":{"id":"048db598"},"source":["We can now use the tranformation matrix $H$ computed using the robust matches to warp our images and create a better-looking panorama."]},{"cell_type":"code","execution_count":null,"id":"de7fd5ea","metadata":{"id":"de7fd5ea"},"outputs":[],"source":["output_shape, offset = get_output_space(img1, [img2], [H])\n","\n","# Warp images into output space\n","img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n","img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n","img1_warped[~img1_mask] = 0     # Return background values to 0\n","\n","img2_warped = warp_image(img2, H, output_shape, offset)\n","img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n","img2_warped[~img2_mask] = 0     # Return background values to 0\n","\n","# Plot warped images\n","plt.subplot(1,2,1)\n","plt.imshow(img1_warped)\n","plt.title('Image 1 warped')\n","plt.axis('off')\n","\n","plt.subplot(1,2,2)\n","plt.imshow(img2_warped)\n","plt.title('Image 2 warped')\n","plt.axis('off')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"612dde50","metadata":{"id":"612dde50"},"outputs":[],"source":["# Merge the two images\n","merged = img1_warped + img2_warped\n","\n","# Track the overlap by adding the masks together\n","overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n","           img2_mask)\n","\n","# Normalize through division by `overlap` - but ensure the minimum is 1\n","normalized = merged / np.maximum(overlap, 1)\n","plt.imshow(normalized)\n","plt.axis('off')\n","plt.title('RANSAC Robust Panorama')\n","plt.show()\n","\n","plt.imshow(imread('solution_ransac_panorama.png'))\n","plt.axis('off')\n","plt.title('RANSAC Robust Panorama Solution')\n","plt.show()"]},{"cell_type":"markdown","id":"d254febf","metadata":{"id":"d254febf"},"source":["## 4. Better Image Merging (10 points)\n","You will notice the blurry region and unpleasant lines in the middle of the final panoramic image. Using a very simple technique called linear blending, we can smooth out a lot of these artifacts.\n","\n","Currently, all the pixels in the overlapping region are weighted equally. However, since the pixels at the left and right ends of the overlap are very well complemented by the pixels in the other image, they can be made to contribute less to the final panorama.\n","\n","Linear blending can be done with the following steps:\n","\n","1. Define left and right margins for blending to occur between\n","2. Define a weight matrix for image 1 such that:\n","    - From the left of the output space to the left margin the weight is 1\n","    - From the left margin to the right margin, the weight linearly decrements from 1 to 0\n","3. Define a weight matrix for image 2 such that:\n","    - From the right of the output space to the right margin the weight is 1\n","    - From the left margin to the right margin, the weight linearly increments from 0 to 1\n","4. Apply the weight matrices to their corresponding images\n","5. Combine the images\n","\n","In **`linear_blend`** below, implement the linear blending scheme to make the panorama look more natural."]},{"cell_type":"code","execution_count":null,"id":"KI6xnj-QkXhu","metadata":{"id":"KI6xnj-QkXhu"},"outputs":[],"source":["def linear_blend(img1_warped, img2_warped):\n","    \"\"\"\n","    Linearly blend img1_warped and img2_warped by following the steps:\n","\n","    1. Define left and right margins (already done for you)\n","    2. Define a weight matrices for img1_warped and img2_warped\n","        np.linspace and np.tile functions will be useful\n","    3. Apply the weight matrices to their corresponding images\n","    4. Combine the images\n","\n","    Args:\n","        img1_warped: Refernce image warped into output space\n","        img2_warped: Transformed image warped into output space\n","\n","    Returns:\n","        merged: Merged image in output space\n","    \"\"\"\n","\n","    out_H, out_W = img1_warped.shape # Height and width of output space\n","    img1_mask = (img1_warped != 0)  # Mask == 1 inside the image\n","    img2_mask = (img2_warped != 0)  # Mask == 1 inside the image\n","\n","    # Find column of middle row where warped image 1 ends\n","    # This is where to end weight mask for warped image 1\n","    right_margin = out_W - np.argmax(np.fliplr(img1_mask)[out_H//2, :].reshape(1, out_W), 1)[0]\n","\n","    # Find column of middle row where warped image 2 starts\n","    # This is where to start weight mask for warped image 2\n","    left_margin = np.argmax(img2_mask[out_H//2, :].reshape(1, out_W), 1)[0]\n","\n","    # Initialize weight maps\n","    w1 = np.zeros((out_H, out_W), dtype=np.float32)\n","    w2 = np.zeros((out_H, out_W), dtype=np.float32)\n","\n","    ### YOUR CODE HERE\n","\n","\n","\n","    ### END YOUR CODE\n","\n","    return merged"]},{"cell_type":"markdown","id":"WscycVJUrjhu","metadata":{"id":"WscycVJUrjhu"},"source":["Now let's see how linear blending improves our result."]},{"cell_type":"code","execution_count":null,"id":"78becc75","metadata":{"id":"78becc75"},"outputs":[],"source":["img1 = imread('uttower1.jpg', as_gray=True)\n","img2 = imread('uttower2.jpg', as_gray=True)\n","\n","# Set seed to compare output against solution\n","np.random.seed(131)\n","\n","# Detect keypoints in both images\n","ec1_keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n","                              threshold_rel=0.05,\n","                              exclude_border=8)\n","ec1_keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n","                              threshold_rel=0.05,\n","                              exclude_border=8)\n","\n","print(\"EC1 keypoints1 shape = \", ec1_keypoints1.shape)\n","print(\"EC1 keypoints2 shape = \", ec1_keypoints2.shape)\n","\n","# Extract features from the corners\n","ec1_desc1 = describe_keypoints(img1, ec1_keypoints1,\n","                           desc_func=simple_descriptor,\n","                           patch_size=16)\n","ec1_desc2 = describe_keypoints(img2, ec1_keypoints2,\n","                           desc_func=simple_descriptor,\n","                           patch_size=16)\n","\n","print(\"EC1 desc1 shape = \", ec1_desc1.shape)\n","print(\"EC1 desc2 shape = \", ec1_desc2.shape)\n","\n","# Match descriptors in image1 to those in image2\n","ec1_matches = match_descriptors(ec1_desc1, ec1_desc2, 0.7)\n","\n","H, robust_matches = ransac(ec1_keypoints1, ec1_keypoints2, ec1_matches, threshold=1)\n","print(\"Robust matches shape = \", robust_matches.shape)\n","print(\"H = \\n\", H)\n","\n","output_shape, offset = get_output_space(img1, [img2], [H])\n","print(\"Output shape:\", output_shape)\n","print(\"Offset:\", offset)\n","\n","# Warp images into output space\n","img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n","img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n","img1_warped[~img1_mask] = 0     # Return background values to 0\n","\n","img2_warped = warp_image(img2, H, output_shape, offset)\n","img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n","img2_warped[~img2_mask] = 0     # Return background values to 0\n","\n","# Merge the warped images using linear blending scheme\n","merged = linear_blend(img1_warped, img2_warped)\n","\n","plt.imshow(merged)\n","plt.axis('off')\n","plt.title('Linear Blend')\n","plt.show()\n","\n","plt.imshow(imread('solution_linear_blend.png'))\n","plt.axis('off')\n","plt.title('Linear Blend Solution')\n","plt.show()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":5}